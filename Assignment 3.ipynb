{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importig notebooks\n",
    "# Kitti Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import FundamentalMatrixTransform\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from numpy import ma\n",
    "from math import ceil\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kitti Handler File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameCalib:\n",
    "    \"\"\"Frame Calibration\n",
    "\n",
    "    Fields:\n",
    "        p0-p3: (3, 4) Camera P matrices. Contains extrinsic and intrinsic parameters.\n",
    "        r0_rect: (3, 3) Rectification matrix\n",
    "        velo_to_cam: (3, 4) Transformation matrix from velodyne to cam coordinate\n",
    "            Point_Camera = P_cam * R0_rect * Tr_velo_to_cam * Point_Velodyne\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.p0 = []\n",
    "        self.p1 = []\n",
    "        self.p2 = []\n",
    "        self.p3 = []\n",
    "        self.r0_rect = []\n",
    "        self.velo_to_cam = []\n",
    "\n",
    "\n",
    "def read_frame_calib(calib_file_path):\n",
    "    \"\"\"Reads the calibration file for a sample\n",
    "\n",
    "    Args:\n",
    "        calib_file_path: calibration file path\n",
    "\n",
    "    Returns:\n",
    "        frame_calib: FrameCalib frame calibration\n",
    "    \"\"\"\n",
    "\n",
    "    data_file = open(calib_file_path, 'r')\n",
    "    data_reader = csv.reader(data_file, delimiter=' ')\n",
    "    data = []\n",
    "\n",
    "    for row in data_reader:\n",
    "        data.append(row)\n",
    "\n",
    "    data_file.close()\n",
    "\n",
    "    p_all = []\n",
    "\n",
    "    for i in range(4):\n",
    "        p = data[i]\n",
    "        p = p[1:]\n",
    "        p = [float(p[i]) for i in range(len(p))]\n",
    "        p = np.reshape(p, (3, 4))\n",
    "        p_all.append(p)\n",
    "\n",
    "    frame_calib = FrameCalib()\n",
    "    frame_calib.p0 = p_all[0]\n",
    "    frame_calib.p1 = p_all[1]\n",
    "    frame_calib.p2 = p_all[2]\n",
    "    frame_calib.p3 = p_all[3]\n",
    "\n",
    "    # Read in rectification matrix\n",
    "    tr_rect = data[4]\n",
    "    tr_rect = tr_rect[1:]\n",
    "    tr_rect = [float(tr_rect[i]) for i in range(len(tr_rect))]\n",
    "    frame_calib.r0_rect = np.reshape(tr_rect, (3, 3))\n",
    "\n",
    "    # Read in velodyne to cam matrix\n",
    "    tr_v2c = data[5]\n",
    "    tr_v2c = tr_v2c[1:]\n",
    "    tr_v2c = [float(tr_v2c[i]) for i in range(len(tr_v2c))]\n",
    "    frame_calib.velo_to_cam = np.reshape(tr_v2c, (3, 4))\n",
    "\n",
    "    return frame_calib\n",
    "\n",
    "\n",
    "class StereoCalib:\n",
    "    \"\"\"Stereo Calibration\n",
    "\n",
    "    Fields:\n",
    "        baseline: distance between the two camera centers\n",
    "        f: focal length\n",
    "        k: (3, 3) intrinsic calibration matrix\n",
    "        p: (3, 4) camera projection matrix\n",
    "        center_u: camera origin u coordinate\n",
    "        center_v: camera origin v coordinate\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.baseline = 0.0\n",
    "        self.f = 0.0\n",
    "        self.k = []\n",
    "        self.center_u = 0.0\n",
    "        self.center_v = 0.0\n",
    "\n",
    "\n",
    "def krt_from_p(p, fsign=1):\n",
    "    \"\"\"Factorize the projection matrix P as P=K*[R;t]\n",
    "    and enforce the sign of the focal length to be fsign.\n",
    "\n",
    "\n",
    "    Keyword Arguments:\n",
    "    ------------------\n",
    "    p : 3x4 list\n",
    "        Camera Matrix.\n",
    "\n",
    "    fsign : int\n",
    "            Sign of the focal length.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    k : 3x3 list\n",
    "        Intrinsic calibration matrix.\n",
    "\n",
    "    r : 3x3 list\n",
    "        Extrinsic rotation matrix.\n",
    "\n",
    "    t : 1x3 list\n",
    "        Extrinsic translation.\n",
    "    \"\"\"\n",
    "    s = p[0:3, 3]\n",
    "    q = np.linalg.inv(p[0:3, 0:3])\n",
    "    u, b = np.linalg.qr(q)\n",
    "    sgn = np.sign(b[2, 2])\n",
    "    b = b * sgn\n",
    "    s = s * sgn\n",
    "\n",
    "    # If the focal length has wrong sign, change it\n",
    "    # and change rotation matrix accordingly.\n",
    "    if fsign * b[0, 0] < 0:\n",
    "        e = [[-1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "        b = np.matmul(e, b)\n",
    "        u = np.matmul(u, e)\n",
    "\n",
    "    if fsign * b[2, 2] < 0:\n",
    "        e = [[1, 0, 0], [0, -1, 0], [0, 0, 1]]\n",
    "        b = np.matmul(e, b)\n",
    "        u = np.matmul(u, e)\n",
    "\n",
    "    # If u is not a rotation matrix, fix it by flipping the sign.\n",
    "    if np.linalg.det(u) < 0:\n",
    "        u = -u\n",
    "        s = -s\n",
    "\n",
    "    r = np.matrix.transpose(u)\n",
    "    t = np.matmul(b, s)\n",
    "    k = np.linalg.inv(b)\n",
    "    k = k / k[2, 2]\n",
    "\n",
    "    # Sanity checks to ensure factorization is correct\n",
    "    if np.linalg.det(r) < 0:\n",
    "        print('Warning: R is not a rotation matrix.')\n",
    "\n",
    "    if k[2, 2] < 0:\n",
    "        print('Warning: K has a wrong sign.')\n",
    "\n",
    "    return k, r, t\n",
    "\n",
    "\n",
    "def get_stereo_calibration(left_cam_mat, right_cam_mat):\n",
    "    \"\"\"Extract parameters required to transform disparity image to 3D point\n",
    "    cloud.\n",
    "\n",
    "    Keyword Arguments:\n",
    "    ------------------\n",
    "    left_cam_mat : 3x4 list\n",
    "                   Left Camera Matrix.\n",
    "\n",
    "    right_cam_mat : 3x4 list\n",
    "                   Right Camera Matrix.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    stereo_calibration_info : Instance of StereoCalibrationData class\n",
    "                              Placeholder for stereo calibration parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    stereo_calib = StereoCalib()\n",
    "    k_left, r_left, t_left = krt_from_p(left_cam_mat)\n",
    "    _, _, t_right = krt_from_p(right_cam_mat)\n",
    "\n",
    "    stereo_calib.baseline = abs(t_left[0] - t_right[0])\n",
    "    stereo_calib.f = k_left[0, 0]\n",
    "    stereo_calib.k = k_left\n",
    "    stereo_calib.center_u = k_left[0, 2]\n",
    "    stereo_calib.center_v = k_left[1, 2]\n",
    "\n",
    "    return stereo_calib\n",
    "\n",
    "\n",
    "class ObjectLabel:\n",
    "    \"\"\"Object Label\n",
    "\n",
    "    Fields:\n",
    "        type (str): Object type, one of\n",
    "            'Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting',\n",
    "            'Cyclist', 'Tram', 'Misc', or 'DontCare'\n",
    "        truncation (float): Truncation level, float from 0 (non-truncated) to 1 (truncated),\n",
    "            where truncated refers to the object leaving image boundaries\n",
    "        occlusion (int): Occlusion level,  indicating occlusion state:\n",
    "            0 = fully visible,\n",
    "            1 = partly occluded,\n",
    "            2 = largely occluded,\n",
    "            3 = unknown\n",
    "        alpha (float): Observation angle of object [-pi, pi]\n",
    "        x1, y1, x2, y2 (float): 2D bounding box of object in the image. (top left, bottom right)\n",
    "        h, w, l: 3D object dimensions: height, width, length (in meters)\n",
    "        t: 3D object centroid x, y, z in camera coordinates (in meters)\n",
    "        ry: Rotation around Y-axis in camera coordinates [-pi, pi]\n",
    "        score: Only for results, indicating confidence in detection, needed for p/r curves.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.type = None  # Type of object\n",
    "        self.truncation = 0.0\n",
    "        self.occlusion = 0\n",
    "        self.alpha = 0.0\n",
    "        self.x1 = 0.0\n",
    "        self.y1 = 0.0\n",
    "        self.x2 = 0.0\n",
    "        self.y2 = 0.0\n",
    "        self.h = 0.0\n",
    "        self.w = 0.0\n",
    "        self.l = 0.0\n",
    "        self.t = (0.0, 0.0, 0.0)\n",
    "        self.ry = 0.0\n",
    "        self.score = 0.0\n",
    "\n",
    "\n",
    "def read_labels(label_dir, sample_name):\n",
    "    \"\"\"Reads in label data file from Kitti Dataset\n",
    "    Args:\n",
    "        label_dir: label directory\n",
    "        sample_name: sample_name\n",
    "    Returns:\n",
    "        obj_list: list of ObjectLabels\n",
    "    \"\"\"\n",
    "\n",
    "    # Check label file\n",
    "    label_path = label_dir + '/{}.txt'.format(sample_name)\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        raise FileNotFoundError('Label file could not be found:', label_path)\n",
    "    if os.stat(label_path).st_size == 0:\n",
    "        return []\n",
    "\n",
    "    labels = np.loadtxt(label_path, delimiter=' ', dtype=str, ndmin=2)\n",
    "    num_rows, num_cols = labels.shape\n",
    "    if num_cols not in [15, 16]:\n",
    "        raise ValueError('Invalid label format')\n",
    "\n",
    "    num_labels = num_rows\n",
    "    is_results = num_cols == 16\n",
    "\n",
    "    obj_list = []\n",
    "    for obj_idx in np.arange(num_labels):\n",
    "        obj = ObjectLabel()\n",
    "\n",
    "        # Fill in the object list\n",
    "        obj.type = labels[obj_idx, 0]\n",
    "        obj.truncation = float(labels[obj_idx, 1])\n",
    "        obj.occlusion = float(labels[obj_idx, 2])\n",
    "        obj.alpha = float(labels[obj_idx, 3])\n",
    "\n",
    "        obj.x1, obj.y1, obj.x2, obj.y2 = (labels[obj_idx, 4:8]).astype(np.float32)\n",
    "        obj.h, obj.w, obj.l = (labels[obj_idx, 8:11]).astype(np.float32)\n",
    "        obj.t = (labels[obj_idx, 11:14]).astype(np.float32)\n",
    "        obj.ry = float(labels[obj_idx, 14])\n",
    "\n",
    "        if is_results:\n",
    "            obj.score = float(labels[obj_idx, 15])\n",
    "        else:\n",
    "            obj.score = 0.0\n",
    "\n",
    "        obj_list.append(obj)\n",
    "\n",
    "    return np.asarray(obj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Estimate Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main_part1():\n",
    "\n",
    "    ################\n",
    "    # Options\n",
    "    ################\n",
    "    # Input dir and output dir\n",
    "\n",
    "    disp_dir = 'data\\\\test\\\\disparity'\n",
    "    output_dir = 'data\\\\test\\\\est_depth'\n",
    "    calib_dir = 'data\\\\test\\calib'\n",
    "    sample_list = ['000011', '000012', '000013', '000014', '000015']\n",
    "    ################\n",
    "\n",
    "    for sample_name in sample_list:\n",
    "        \n",
    "        # Read disparity map\n",
    "        disp_dir_path = '.\\\\'+disp_dir +'\\\\' + sample_name + '.png'\n",
    "        disparity_map = cv2.imread(disp_dir_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Uncomment below code to view the disparity maps in Jupyter Notebook\n",
    "        #plt.imshow(disp_map, cmap = 'gray')\n",
    "        #plt.show()\n",
    "        \n",
    "        # Read calibration info\n",
    "        frame_calib_path = calib_dir +'\\\\' + sample_name + '.txt'\n",
    "        frame_calib = read_frame_calib(frame_calib_path)\n",
    "        \n",
    "        # Calculate depth (z = f*B/disp)\n",
    "        stereo_calib = get_stereo_calibration(frame_calib.p2, frame_calib.p3)\n",
    "        baseline = stereo_calib.baseline\n",
    "        f = stereo_calib.f\n",
    "        focus_base = baseline*f\n",
    "        \n",
    "        depth = np.zeros(disparity_map.shape, dtype = float)\n",
    "        \n",
    "        #print(disparity_map.shape)\n",
    "        \n",
    "        # We calculate depth using disparity ONLY for values in disparity matrix where disparity is not equal to 0\n",
    "        depth[disparity_map != 0] = focus_base / disparity_map[disparity_map != 0]\n",
    "        depth = depth.astype(int)\n",
    "\n",
    "        #print(depth.shape)\n",
    "        #print(depth[disparity_map != 0])\n",
    "        #print(depth)\n",
    "        \n",
    "        depth[np.logical_or(depth > 80, depth < 0.1)] = 0\n",
    "        \n",
    "        # Save depth map\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(output_dir)\n",
    "            cv2.imwrite(output_dir+'\\\\'+sample_name+'.png',depth)\n",
    "        except:\n",
    "            cv2.imwrite(output_dir+'\\\\'+sample_name+'.png',depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_main_part1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing Code for Part 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_main_part1():\n",
    "\n",
    "    ################\n",
    "    # Options\n",
    "    ################\n",
    "    # Input dir and output dir\n",
    "    disp_dir_train = 'data\\\\train\\\\disparity'\n",
    "    output_dir_train = 'data\\\\train\\\\est_depth'\n",
    "    calib_dir_train = 'data\\\\train\\\\calib'\n",
    "    gt_depth_dir_train = 'data\\\\train\\\\gt_depth'\n",
    "    sample_list_train = ['000001', '000002', '000003', '000004','000005', '000006', '000007', '000008', '000009', '000010']\n",
    "    \n",
    "    disp_dir = 'data\\\\test\\\\disparity'\n",
    "    output_dir = 'data\\\\test\\\\est_depth'\n",
    "    calib_dir = 'data\\\\test\\calib'\n",
    "    sample_list = ['000011', '000012', '000013', '000014', '000015']\n",
    "    ################\n",
    "\n",
    "    for sample_name in sample_list_train:\n",
    "        \n",
    "        # Read disparity map\n",
    "        disp_dir_path = '.\\\\'+disp_dir_train +'\\\\' + sample_name + '.png'\n",
    "        disparity_map = cv2.imread(disp_dir_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        gt_depth_path = gt_depth_dir_train+'\\\\'+sample_name+'.png'\n",
    "        gt_depth_map =  cv2.imread(gt_depth_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #plt.imshow(disp_map, cmap = 'gray')\n",
    "        #plt.show()\n",
    "        \n",
    "        # Read calibration info\n",
    "        frame_calib_path = calib_dir_train +'\\\\' + sample_name + '.txt'\n",
    "        frame_calib = read_frame_calib(frame_calib_path)\n",
    "        \n",
    "        # Calculate depth (z = f*B/disp)\n",
    "        stereo_calib = get_stereo_calibration(frame_calib.p2, frame_calib.p3)\n",
    "        baseline = stereo_calib.baseline\n",
    "        f = stereo_calib.f\n",
    "        focus_base = baseline*f\n",
    "        \n",
    "        depth = np.zeros(disparity_map.shape, dtype = float)\n",
    "        \n",
    "        #print(disparity_map.shape)\n",
    "        \n",
    "        depth[disparity_map != 0] = focus_base / disparity_map[disparity_map != 0]\n",
    "        depth = depth.astype(int)\n",
    "\n",
    "        #print(depth.shape)\n",
    "        #print(depth[disparity_map != 0])\n",
    "        \n",
    "        #print(depth)\n",
    "        \n",
    "        depth[np.logical_or(depth > 80, depth < 0.1)] = 0\n",
    "        \n",
    "        #Checking RMSE\n",
    "        \n",
    "        \n",
    "        plt.imshow(gt_depth_map)\n",
    "        plt.imshow(depth)\n",
    "        plt.show()\n",
    "        rmse = np.sqrt(np.mean((depth-gt_depth_map)**2))\n",
    "        print(rmse)\n",
    "        \n",
    "        # Save depth map\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(output_dir_train)\n",
    "            cv2.imwrite(output_dir_train+'\\\\'+sample_name+'.png',depth)\n",
    "        except:\n",
    "            cv2.imwrite(output_dir_train+'\\\\'+sample_name+'.png',depth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    train_main_part1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the YOLOV3 section we used a threshold value of 0.5 and a confidence value of 0.50. This gave us the best results and keeping threshold to minimuma for the IOUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 1.084015 seconds\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.828163 seconds\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.816754 seconds\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.870564 seconds\n",
      "[INFO] loading YOLO from disk...\n",
      "[INFO] YOLO took 0.941253 seconds\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python part2_yolo.py --image images/baggage_claim.jpg --yolo yolo\n",
    "\n",
    "# import the necessary packages\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# OPTIONS\n",
    "###########################################################\n",
    "\n",
    "image_path = 'data\\\\test\\\\left\\\\'\n",
    "yolo_dir = 'yolo'\n",
    "sample_list = ['000011', '000012', '000013', '000014', '000015']\n",
    "# minimum probability to filter weak detections\n",
    "confidence_th = 0.50\n",
    "\n",
    "# threshold when applyong non-maxima suppression\n",
    "threshold = 0.50\n",
    "###########################################################\n",
    "\n",
    "for sample_name in sample_list:\n",
    "    \n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    labelsPath = os.path.sep.join([yolo_dir, \"coco.names\"])\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "                               dtype=\"uint8\")\n",
    "\n",
    "    # derive the paths to the YOLO weights and model configurationY\n",
    "    weightsPath = os.path.sep.join([yolo_dir, \"yolov3.weights\"])\n",
    "    configPath = os.path.sep.join([yolo_dir, \"yolov3.cfg\"])\n",
    "\n",
    "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "    print(\"[INFO] loading YOLO from disk...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "    # load our input image and grab its spatial dimensions\n",
    "    image = cv2.imread(image_path+sample_name+'.png')\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # show timing information on YOLO\n",
    "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > confidence_th:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "    # boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence_th,\n",
    "                            threshold)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('data\\\\test\\\\yolo_bbox\\\\')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('data\\\\test\\\\yolo_bbox\\\\'+sample_name+'.txt','w') as f:\n",
    "        \n",
    "        # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            # loop over the indexes we are keeping\n",
    "            for i in idxs.flatten():\n",
    "                # extract the bounding box coordinates\n",
    "                (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "                # draw a bounding box rectangle and label on the image\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "                cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5, color, 2)\n",
    "                \n",
    "                if LABELS[classIDs[i]] == 'car' or LABELS[classIDs[i]] == 'Car':\n",
    "                    if x < 0:\n",
    "                        x = 0\n",
    "                    elif y < 0:\n",
    "                        y = 0\n",
    "                    _ = 0\n",
    "                    f.write('{} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {}\\n'.format(LABELS[classIDs[i]],_,_,_,x,y,x + w,y + h,_,_,_,_,_,_,_,_))   \n",
    "                                                                                  \n",
    "                    \n",
    "    # show the output image\n",
    "    cv2.imshow(sample_name, image)\n",
    "    cv2.imwrite(os.path.join('data\\\\test\\\\yolo_bbox\\\\',sample_name + \".png\"),image)\n",
    "    \n",
    "    plt.show()\n",
    "    cv2.waitKey(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yolo Part 2 Training Code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# USAGE\n",
    "# python part2_yolo.py --image images/baggage_claim.jpg --yolo yolo\n",
    "\n",
    "# import the necessary packages\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# OPTIONS\n",
    "###########################################################\n",
    "image_path = 'data\\\\train\\\\left\\\\'\n",
    "yolo_dir = 'yolo'\n",
    "sample_list_train = ['000001', '000002', '000003', '000004','000005', '000006', '000007', '000008', '000009', '000010']\n",
    "# minimum probability to filter weak detections\n",
    "confidence_th = 0.6\n",
    "\n",
    "# threshold when applyong non-maxima suppression\n",
    "threshold = 0.5\n",
    "###########################################################\n",
    "\n",
    "for sample_name in sample_list_train:\n",
    "    \n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    labelsPath = os.path.sep.join([yolo_dir, \"coco.names\"])\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "                               dtype=\"uint8\")\n",
    "\n",
    "    # derive the paths to the YOLO weights and model configurationY\n",
    "    weightsPath = os.path.sep.join([yolo_dir, \"yolov3.weights\"])\n",
    "    configPath = os.path.sep.join([yolo_dir, \"yolov3.cfg\"])\n",
    "\n",
    "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "    print(\"[INFO] loading YOLO from disk...\")\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "    # load our input image and grab its spatial dimensions\n",
    "    image = cv2.imread(image_path+sample_name+'.png')\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # determine only the *output* layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # construct a blob from the input image and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "    # associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # show timing information on YOLO\n",
    "    print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, confidences, and\n",
    "    # class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > confidence_th:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "    # boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence_th,\n",
    "                            threshold)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('data\\\\train\\\\yolo_gt_labels\\\\')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('data\\\\train\\\\yolo_gt_labels\\\\'+sample_name+'.txt','w') as f:\n",
    "        \n",
    "        # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            # loop over the indexes we are keeping\n",
    "            for i in idxs.flatten():\n",
    "                # extract the bounding box coordinates\n",
    "                (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "                # draw a bounding box rectangle and label on the image\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "                cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5, color, 2)\n",
    "                \n",
    "                if LABELS[classIDs[i]] == 'car' or LABELS[classIDs[i]] == 'Car':\n",
    "                    if x < 0:\n",
    "                        x = 0\n",
    "                    elif y < 0:\n",
    "                        y = 0\n",
    "                    \n",
    "                    _ = 0\n",
    "                    f.write('{} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {}\\n'.format(LABELS[classIDs[i]],_,_,_,x,y,x + w,y + h,_,_,_,_,_,_,_,_))   \n",
    "                    \n",
    "    # show the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    plt.show()\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    \n",
    "  '''     obj.type = labels[obj_idx, 0]\n",
    "        obj.truncation = float(labels[obj_idx, 1])\n",
    "        obj.occlusion = float(labels[obj_idx, 2])\n",
    "        obj.alpha = float(labels[obj_idx, 3])\n",
    "\n",
    "        obj.x1, obj.y1, obj.x2, obj.y2 = (labels[obj_idx, 4:8]).astype(np.float32)\n",
    "        obj.h, obj.w, obj.l = (labels[obj_idx, 8:11]).astype(np.float32)\n",
    "        obj.t = (labels[obj_idx, 11:14]).astype(np.float32)\n",
    "        obj.ry = float(labels[obj_idx, 14])'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For segmentation we have 2 methods. \n",
    "1. First option used is the mask library in numpy and calculating mask using that\n",
    "2. The second method was just using numpy matrices\n",
    "\n",
    "\n",
    "For distance threshold when calculating the mask there were 2 threshold set.\n",
    "1. Minimum threshold \n",
    "2. Maximum threshold\n",
    "\n",
    "These threshold based on the value difference between the depth map and the average depth map of the detected bounding box\n",
    "selected the pixel and changed them to 0 (black)\n",
    "\n",
    "To select the min/max threshold values we defined them based on average depth values based on where they lied in the range.\n",
    "Average depth was split into the following ranges:\n",
    "1. 0 to 8\n",
    "2. 8 to 10\n",
    "3. 10 to 15\n",
    "4. 15 to 20\n",
    "5. 20 to 30 \n",
    "6. 30 to 40\n",
    "7. 40 to 50\n",
    "8. 50 to 60\n",
    "9. 60+\n",
    "\n",
    "Based on the above ranges different threshold values were set.\n",
    "\n",
    "\n",
    "Please note: that a signle depth distance threshold value was also experimented with however, for those precision and recall values were not that great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining fucntions to calculate Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_precision(target, ground_truth):\n",
    "    # TP: True Positive\n",
    "    # FP: False Positive\n",
    "    ground_truth[ground_truth < 10 ]=0\n",
    "    TP = np.sum(target == ground_truth)\n",
    "    #print(\"TP: \", TP)\n",
    "    \n",
    "    FP = np.sum([ tr == 0 and gt == 255 for (tr,gt) in zip(target,ground_truth)])\n",
    "    #print(\"FP: \", FP)\n",
    "    \n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def map_recall(target, ground_truth):\n",
    "    # TP: True Positive\n",
    "    # FP: False Negative\n",
    "    #ground_truth[ground_truth < 10 ]=0\n",
    "    TP = np.sum(target == ground_truth)\n",
    "    FN = np.sum([ tr == 255 and gt == 0 for (tr,gt) in zip(target,ground_truth)])\n",
    "    \n",
    "    return TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main_part3():\n",
    "\n",
    "    ################\n",
    "    # Options\n",
    "    ################\n",
    "    # Input dir and output dir\n",
    "    \n",
    "   \n",
    "    depth_dir = 'data\\\\test\\\\est_depth'\n",
    "    label_dir = 'data\\\\test\\\\yolo_bbox'\n",
    "    image_dir = 'data\\\\test\\\\left'\n",
    "    output_dir = 'data\\\\test\\\\est_segmentation'\n",
    "   \n",
    "    sample_list = ['000011', '000012', '000013', '000014', '000015']\n",
    "\n",
    "    ################\n",
    "\n",
    "    for sample_name in sample_list:\n",
    "        \n",
    "        # Read depth map\n",
    "        depth_path = depth_dir+'\\\\'+sample_name+'.png'\n",
    "        depth_map =  cv2.imread(depth_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #print(depth_map)        \n",
    "        \n",
    "        # Print the image Name\n",
    "        print(\"Image: \",sample_name)\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        count = 0\n",
    "        \n",
    "        # Discard depths less than 10cm from the camera\n",
    "        depth_map[depth_map < 0.1] = 0\n",
    "        \n",
    "        # Read 2d bbox\n",
    "        bbox = read_labels(label_dir, sample_name)\n",
    "        image_mask = np.ones_like(depth_map) * 255\n",
    "        \n",
    "        # For each bbox\n",
    "        for label in bbox:\n",
    "            \n",
    "            #check if the label is Car or not\n",
    "            if ((label.type == 'Car')or (label.type == 'car')):\n",
    "                \n",
    "                #print(sample_name+\" \"+label.type)\n",
    "                \n",
    "                # Estimate the average depth of the objects\n",
    "                \n",
    "                # Extract the detected car's location from the depth map\n",
    "                bbox_depth = depth_map[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1]\n",
    "                \n",
    "                # Calculate average of the detected car's depth map (within the bounding boxes)\n",
    "                try:\n",
    "                    average_depth = np.nanmean(np.nanmean(np.where(bbox_depth!=0,bbox_depth,np.nan),1))\n",
    "                except:\n",
    "                    average_depth = np.mean(bbox_depth, axis=(0 , 1))\n",
    "                if(math.isnan(average_depth)):\n",
    "                    continue\n",
    "                print(\"Average Depth: \", average_depth)\n",
    "\n",
    "                # Find the pixels within a certain distance from the centroid\n",
    "                \n",
    "                # declare requried variable to calculate distance for pixels\n",
    "                distance_range_min = 0\n",
    "                distance_range_max = 0\n",
    "                \n",
    "                # Selecting istance threshold values (min/max) based on average depth value\n",
    "                \n",
    "                avg_depth = int(ceil(average_depth))\n",
    "                if avg_depth <= 8:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 3\n",
    "                elif avg_depth > 8 and avg_depth <= 10:\n",
    "                    distance_range_min = 3\n",
    "                    distance_range_max = 2\n",
    "                elif avg_depth > 10 and avg_depth <= 15:\n",
    "                    distance_range_min = 3\n",
    "                    distance_range_max = 3\n",
    "                elif avg_depth > 15 and avg_depth <= 20:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                elif avg_depth > 20 and avg_depth <= 30:\n",
    "                    distance_range_min = 8\n",
    "                    distance_range_max = 8\n",
    "                elif avg_depth > 30 and avg_depth <= 40:\n",
    "                    distance_range_min = 8\n",
    "                    distance_range_max = 9\n",
    "                elif avg_depth > 40 and avg_depth <= 50:\n",
    "                    distance_range_min = 5\n",
    "                    distance_range_max = 7\n",
    "                elif avg_depth > 50 and avg_depth <= 60:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                else:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                    \n",
    "                depth_min_range = average_depth - distance_range_min\n",
    "                depth_max_range = average_depth + distance_range_max\n",
    "                \n",
    "                # Checking the threshold values selected\n",
    "                #print(\"Threshold Values: \",depth_min_range,\" \",depth_max_range)\n",
    "                \n",
    "                # Checking the difference between the bbox_depth and averages depth value\n",
    "                #print(\"Difference\")\n",
    "                #print((bbox_depth - average_depth))\n",
    "                \n",
    "                # looking at the image of the difference\n",
    "                #plt.imshow((bbox_depth - average_depth), cmap = 'gray')\n",
    "                #plt.show()     \n",
    "\n",
    "                # Using Numpy to mask matrix positions that are within range.\n",
    "                box_mask = ma.masked_inside(bbox_depth, depth_min_range, depth_max_range)\n",
    "                \n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(mask1,cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(mask1)\n",
    "                \n",
    "                # Apply Mask to change the calculate the mask location based on integer values\n",
    "                box_mask_2 = box_mask.mask.astype(int)\n",
    "\n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(box_mask_2,cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(box_mask_2)\n",
    "                \n",
    "                # Convert image into Black and White for the segmentaion mask\n",
    "                box_mask_2[box_mask_2 == 0] = 255\n",
    "                box_mask_2[box_mask_2 == 1] = 0\n",
    "                \n",
    "                #image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1] = int_mask\n",
    "                \n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(box_mask_2,cmap='gray')\n",
    "                #plt.show()\n",
    "                \n",
    "                #print(\"Mask section Shape: \",box_mask_2.shape)\n",
    "                #print(box_mask_2)               \n",
    "                \n",
    "                \n",
    "                mask_shape = box_mask_2.shape\n",
    "                #print(mask_shape)\n",
    "                \n",
    "\n",
    "                check = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()\n",
    "                box_mask_2 = box_mask_2.flatten()\n",
    "                image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1] = np.array([b if a!=0 else a for a,b in zip(check,box_mask_2)]).reshape(mask_shape)\n",
    "                est = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()          \n",
    "\n",
    "\n",
    "        plt.imshow(image_mask,cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "            \n",
    "        # Save the segmentation mask\n",
    "        try:\n",
    "            os.mkdir(output_dir)\n",
    "            cv2.imwrite(os.path.join(output_dir,sample_name + \".png\"),image_mask)\n",
    "        except:\n",
    "            cv2.imwrite(os.path.join(output_dir,sample_name + \".png\"),image_mask)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  000011\n",
      "Average Depth:  40.486272488755624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f9062c15c80b>:54: RuntimeWarning: Mean of empty slice\n",
      "  average_depth = np.nanmean(np.nanmean(np.where(bbox_depth!=0,bbox_depth,np.nan),1))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  000012\n",
      "Average Depth:  27.19789174601668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  000013\n",
      "Average Depth:  11.401811805961424\n",
      "Average Depth:  19.832632345359055\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAACICAYAAADtePALAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASO0lEQVR4nO3df3DU9Z3H8eebJEBMoAQIyiUoqMwh3Hi0yQi11HFqz4J2Cv6hxVrFuTj0D+35q6MIzFDbYWrv8FfrlRsmcPw4rGL5ORQGWk6mXqtCokAJFExFIAcHYSTyM7/f98d+STewSTYkm9395vWY2dnv97Pf7+77HcIru9/9/jB3R0REwqdPsgsQEZHEUMCLiISUAl5EJKQU8CIiIaWAFxEJKQW8iEhIJSzgzWyymR0ws0ozm5Wo1xERkdgsEfvBm1kGcBD4J6AK2Ak86O77uv3FREQkpkS9g78NqHT3T929HngLmJqg1xIRkRgSFfAFwNGo+apgTEREekhmgp7XYoy12hZkZjOBmQA5OTlFY8aMSVApIiLhVF5efsrd89t6PFEBXwWMiJovBI5FL+Dui4BFAMXFxV5WVpagUkREwsnMDrf3eKI20ewERpvZKDPrC0wHNiTotUREJIaEvIN390YzewLYAmQAS9y9IhGvJSIisSVqEw3uvgnYlKjnFxGR9ulIVhGRkFLAi4iElAJeRCSkFPAiIiGlgBcRCSkFvIhISCngRURCSgEvIhJSCngRkZBSwIuIhJQCXkQkpBTwIiIhpYAXEQkpBbyISEgp4EVEQkoBLyISUgp4EZGQUsCLiIRUhwFvZkvM7KSZ7Y0aG2xmvzOzT4L7vKjHXjCzSjM7YGbfSlThIiLSvnjewS8FJl82NgvY5u6jgW3BPGY2FpgOjAvW+ZWZZXRbtSIiErcOA97d/wB8ftnwVGBZML0MmBY1/pa717n7IaASuK2bahURkU642m3w17r7cYDgflgwXgAcjVquKhgTEZEe1t1fslqMMY+5oNlMMyszs7Lq6upuLkNERK424E+Y2XCA4P5kMF4FjIharhA4FusJ3H2Ruxe7e3F+fv5VliEiIm252oDfAMwIpmcA66PGp5tZPzMbBYwGdnStRBERuRqZHS1gZr8G7gSGmlkVMA94CVhlZiXAEeB+AHevMLNVwD6gEXjc3ZsSVLuIiLSjw4B39wfbeOiuNpafD8zvSlEiItJ1OpJVRCSkFPAiIiGlgBcRCSkFvIhISCngRURCSgEvIhJSCngRkZBSwIuIhJQCXkQkpBTwIiIhpYAXEQkpBbyISEgp4EVEQqrDs0mKSHg1NTXx+9//np07d8a1/JAhQ7j//vsZOnRogiuT7qCAF+mlmpubef3115kzZw61tbVxrWNmlJeXs2jRIvr00QaAVNfrA/7cuXOcOXOmZT4rK4tUuoRgfX09p06dIjs7m7y8vFaPnTlzhnPnzrUay8zMJD8/H7MrL4/r7i3jl/oePHgwffv21X/WXqiiooKf/vSncYc7RH6HNm3axMGDBxkzZkwCq5Pu0OsDfvny5cydO7dlftq0aZSWlqZM4J08eZI777yTvLw8VqxYwfDhwwGoqalh5syZV3y0zs7O5sc//jG33HJLq/H33nuPiooKHn74YZqamliwYAG7du3ipptu4pprrqGoqIhp06a16vu6665r+WOXkZFBbm5ugruVntLQ0MAvf/lLampqOr3u8ePHee6551i2bNkVbzokxbh70m9FRUWeLD/5yU8caLmNHz/ea2trk1ZPtObmZl+xYoX379/fAc/Ly/PCwkIvLCz0oUOHtqo7+mZmnpGR0ep26bE+ffp4nz59Yq53+TqDBg1qeb1x48b58uXL/cKFC8n+sUgXlZWV+bPPPuv9+vVr83eoo5uZ+dy5c5PdSq8HlHk72RrPNVlHAMuB64BmYJG7v25mg4G3gZHAZ8AD7n46WOcFoARoAv7F3bdc1V+fBHN3duxofU3wESNGkJmZvA82jY2NrF+/nm3btnHx4kXWrl3b8hH69OnTnD59usPncHeammJfCre5ubnN9aLXyczM5Prrr2fPnj0AVFVVUVJSwpIlS3jppZeYMGFCZ9qSFHHx4kVKSkrYvXt3l57H3dm1a1c3VSWJEs92iEbgWXe/BZgIPG5mY4FZwDZ3Hw1sC+YJHpsOjAMmA78ys4xEFN8dLg/CwsJCMjKSU25TUxOvvfYaDz30EAsXLmTp0qV88cUXSall/PjxPPbYY6022TQ0NLB9+3YeeOAB3n///aTUJV2XlZWV7BKkh3QY8O5+3N0/CqbPAvuBAmAqsCxYbBkwLZieCrzl7nXufgioBG7r7sIT5fbbb+/R12toaKC2tpaamhpefvll5s6dS11dXY/WEMvHH3/Mj370o5jv+I8cOcKDDz4Y9651kjqys7NZtGgRN954Y7JLkR7QqW8SzWwk8GXgQ+Badz8OkT8CwLBgsQLgaNRqVcFYyjl//jxHjhxJ2ut/8MEHTJkyhaKiIoqKipg9e3ZKhDtEPk3U19e3+fjhw4d5+umnOXnyZA9WJd3h1ltv1Sa2XiLugDezXGA18JS7n2lv0RhjHuP5ZppZmZmVVVdXx1tGt6qtreXUqVOtxt57770eee1z587x5JNPsm3bNvbt28enn37a5nbzVPXHP/6R+fPnp13dvVl9fT2zZ89mw4YNyS5FekBcAW9mWUTCfaW7rwmGT5jZ8ODx4cClt3JVwIio1QuBY5c/p7svcvdidy9O1n7n/fv3v2Kf961bt3Lo0KGW+ebmZt555x02btxI5Evr1mpra1m2bBlVVVVxvaa7c/jwYV599VU++uijrjWQApYsWUJ5eXmyy5A41dfXs3r1as6fP5/sUqQHdBjwFjkyZjGw391fiXpoAzAjmJ4BrI8an25m/cxsFDAaaL2rSorIyclh9OjRrcY+++wz5syZ03IA0datWykpKeF73/seK1euvCLkL1y4wNy5c3nmmWe4ePFiu6/X3NzM4sWLmTBhAvPmzaOxsbF7G0qCc+fOsX379mSXISIxxLM/4NeAh4E/m9ml/aJmAy8Bq8ysBDgC3A/g7hVmtgrYR2QPnMfdPSU/w5sZEydOZO3ata3G3377bS5evMiYMWN45513OHv2LADPP/88kyZNYuTIkUAk3DIyMigoKGDNmjXcfPPNzJ8/P+ZRpABr1qzhqaeeCt27p/Z2vRSR5Okw4N39f4i9XR3grjbWmQ/M70JdPSbWYdrNzc2sW7fuivFjx47x4osv8sQTT1BeXs7ChQsB2LNnD83NzbzxxhtMmTKFr3/960Bkv/WjR48yYMAARowYweLFi0MX7iKSunr1qQrq6urYsqVzx2AtXbqUN998k8bGxiveuZ49e5Z58+axYMECTpw4wezZs9m/fz+5ubnccccdod13/MSJE8kuQURi6LUBf+lIvEtHanZGe7sPvvvuu0yaNInGxkYaGhqAyB+SyzcDhcmuXbtoampK2gFi0jl5eXkMGTKky88zYMCAbqhGEqlXBvyf/vQnfvazn7Fr164rzsbYHTr6sjVsKioqqKqq4oYbbkh2KdKBnJwcNm/e3C27tvbr168bKpJE6pUBv3v3bjZu3JjsMkLj1KlT7N+/XwGfBsxMF+voRVLjnLiS1gYOHEhBQUoerCzSqyngpcvuu+8+xo4dm+wyROQyCnjpkgEDBvDDH/5QX7CKpCAFvHRJQ0MDBw4coLq6uuWAMBFJDQp46ZLa2loeffRRxo0bx+zZs5NdjohEUcBLl9XX11NdXd3q4uUiknwKeBGRkFLAi4iEVK880CnRMjIyyM/PJzc3F4icd378+PEte5oMGzaMW2+9lX379vHqq6/GPOGZiEhXKeC7YODAgTzyyCPcdNNN7Nixg9WrV/OlL32J0tJSiouLGTRoEBA5ejA7O/uK9c+ePcvmzZt1dXoRSQgFfBfcc889vPzyy/Tt25eamhp27txJQUEBw4YNo6qqivr6eoYPHx7znB0XLlzgBz/4AXv37k1C5SLSGyjgu2DNmjXU1tZSUlICwNGjR6msrGw5H/zAgQOZMGECK1euJC8vj6amJpqamsjKyuLIkSP89re/DcVVnUQkNSngu6C+vp5169axadMm+vfvT11dHUBLaH/++eds2bKFOXPmcN9991FaWsrBgweZOnUq9957L1lZWcksX0RCTgHfDerr69s8R3xzczMLFy6ktLS05fzwe/bs4ZVXXtGRnyKSUPFcdLu/me0ws91mVmFmLwbjg83sd2b2SXCfF7XOC2ZWaWYHzOxbiWwgXVwKd4iEvsJdRBItnv3g64BvuPs/AuOByWY2EZgFbHP30cC2YB4zGwtMB8YBk4FfmZnORCUi0sPiuei2A5cue5QV3ByYCtwZjC8DtgPPB+NvuXsdcMjMKoHbgJS5IGnfvn0ZOHBgsssInVi7gopI8sS1DT54B14O3Az8u7t/aGbXuvtxAHc/bmbDgsULgA+iVq8KxlLGd7/7Xe6+++5klxE6OTk5yS5BRKLEFfDu3gSMN7NBwFoz+4d2FrdYT3HFQmYzgZkA119/fTxldJvc3NyWo0xFRMKqU+eicfcaIptiJgMnzGw4QHB/MlisChgRtVohcCzGcy1y92J3L87Pz7+K0kVEpD3x7EWTH7xzx8yygW8CfwE2ADOCxWYA64PpDcB0M+tnZqOA0cCO7i5cRETaF88mmuHAsmA7fB9glbtvNLP3gVVmVgIcAe4HcPcKM1sF7AMagceDTTwiItKDLLKTTHIVFxd7WVlZsssQEUkrZlbu7sVtPa7zwYuIhJQCXkQkpBTwIiIhpYAXEQkpBbyISEgp4EVEQkoBLyISUgp4EZGQUsCLiISUAl5EJKQU8CIiIaWAFxEJKQW8iEhIKeBFREJKAS8iElIpcT54MzsLHEh2HQkwFDiV7CISQH2lF/WVXjrT1w3u3uY1T+O66HYPONDeSevTlZmVqa/0ob7Si/rqmDbRiIiElAJeRCSkUiXgFyW7gARRX+lFfaUX9dWBlPiSVUREul+qvIMXEZFulvSAN7PJZnbAzCrNbFay6+kMMxthZu+a2X4zqzCzJ4PxwWb2OzP7JLjPi1rnhaDXA2b2reRV3z4zyzCzj81sYzAfhp4GmdlvzOwvwb/ZV0PS19PB799eM/u1mfVPx77MbImZnTSzvVFjne7DzIrM7M/BY78wM+vpXqK10de/Bb+He8xsrZkNinqs+/py96TdgAzgr8CNQF9gNzA2mTV1sv7hwFeC6QHAQWAs8K/ArGB8FvDzYHps0GM/YFTQe0ay+2ijt2eAN4GNwXwYeloGPBZM9wUGpXtfQAFwCMgO5lcBj6ZjX8AdwFeAvVFjne4D2AF8FTBgMzAlBfu6G8gMpn+eqL6S/Q7+NqDS3T9193rgLWBqkmuKm7sfd/ePgumzwH4i/+GmEgkTgvtpwfRU4C13r3P3Q0AlkZ9BSjGzQuBeoDRqON17GkjkP9piAHevd/ca0ryvQCaQbWaZwDXAMdKwL3f/A/D5ZcOd6sPMhgMD3f19j6Ti8qh1kiJWX+6+1d0bg9kPgMJgulv7SnbAFwBHo+argrG0Y2YjgS8DHwLXuvtxiPwRAIYFi6VLv68BzwHNUWPp3tONQDXwn8Gmp1IzyyHN+3L3/wUWAEeA48AX7r6VNO8rSmf7KAimLx9PZf9M5B05dHNfyQ74WNuQ0m63HjPLBVYDT7n7mfYWjTGWUv2a2beBk+5eHu8qMcZSqqdAJpGPyQvd/cvAeSIf+duSFn0F26SnEvk4/3dAjpl9v71VYoylXF9xaKuPtOrPzOYAjcDKS0MxFrvqvpId8FXAiKj5QiIfL9OGmWURCfeV7r4mGD4RfKQiuD8ZjKdDv18DvmNmnxHZZPYNM/sv0rsniNRZ5e4fBvO/IRL46d7XN4FD7l7t7g3AGuB20r+vSzrbRxV/29wRPZ5yzGwG8G3goWCzC3RzX8kO+J3AaDMbZWZ9genAhiTXFLfgW+zFwH53fyXqoQ3AjGB6BrA+any6mfUzs1HAaCJfnKQMd3/B3QvdfSSRf4//dvfvk8Y9Abj7/wFHzezvg6G7gH2keV9ENs1MNLNrgt/Hu4h8F5TufV3SqT6CzThnzWxi8PN4JGqdlGFmk4Hnge+4+4Woh7q3r2R+uxz80bqHyN4nfwXmJLueTtY+icjHpD3AruB2DzAE2AZ8EtwPjlpnTtDrAZL87X4c/d3J3/aiSfuegPFAWfDvtQ7IC0lfLwJ/AfYCK4jsgZF2fQG/JvI9QgORd6wlV9MHUBz8LP4KvEFwQGeK9VVJZFv7pdz4j0T0pSNZRURCKtmbaEREJEEU8CIiIaWAFxEJKQW8iEhIKeBFREJKAS8iElIKeBGRkFLAi4iE1P8DE94WoSjfwbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  000014\n",
      "Average Depth:  51.89955553599718\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  000015\n",
      "Average Depth:  11.795729013254787\n",
      "Average Depth:  21.536995376279524\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_main_part3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation TRAINING SECTION (Commented)\n",
    "#### Part3: Training Code Method 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_main1():\n",
    "\n",
    "    ################\n",
    "    # Options\n",
    "    ########a########\n",
    "    # Input dir and output dir\n",
    "    \n",
    "   \n",
    "    depth_dir = 'data\\\\train\\\\est_depth'\n",
    "    label_dir = 'data\\\\train\\\\gt_labels'\n",
    "    image_dir = 'data\\\\train\\\\left'\n",
    "    output_dir = 'data\\\\train\\\\est_segmentation'\n",
    "    gt_seg_dir = 'data\\\\train\\\\gt_segmentation'\n",
    "    sample_list = ['000001', '000002', '000003', '000004','000005', '000006', '000007', '000008', '000009', '000010']\n",
    "    #sample_list = ['000009']\n",
    "    ################\n",
    "\n",
    "    for sample_name in sample_list:\n",
    "        \n",
    "        # Read depth map\n",
    "        depth_path = depth_dir+'\\\\'+sample_name+'.png'\n",
    "        depth_map =  cv2.imread(depth_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #print(depth_map)\n",
    "        #break\n",
    "        \n",
    "        # Read Ground Truth Segmentation Image:\n",
    "        gt_seg_path  = gt_seg_dir+'\\\\'+sample_name+'.png'\n",
    "        gt_seg = cv2.imread(gt_seg_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Print the image Name\n",
    "        print(\"Image: \",sample_name)\n",
    "        \n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        count = 0\n",
    "        \n",
    "        # Discard depths less than 10cm from the camera\n",
    "        depth_map[depth_map < 0.1] = 0\n",
    "        \n",
    "        # Read 2d bbox\n",
    "        bbox = read_labels(label_dir, sample_name)\n",
    "        image_mask = np.ones_like(depth_map) * 255\n",
    "        # For each bbox\n",
    "        for label in bbox:\n",
    "            \n",
    "            if label.type == 'Car':\n",
    "                #print(sample_name+\" \"+label.type)\n",
    "                \n",
    "                # Estimate the average depth of the objects\n",
    "                \n",
    "                bbox_depth = depth_map[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1]\n",
    "\n",
    "                try:\n",
    "                    average_depth = np.nanmean(np.nanmean(np.where(bbox_depth!=0,bbox_depth,np.nan),1))\n",
    "                except:\n",
    "                    average_depth = np.mean(bbox_depth, axis=(0 , 1))\n",
    "                print(\"Average Depth: \", average_depth)\n",
    "\n",
    "                # Find the pixels within a certain distance from the centroid\n",
    "                distance_range_min = 0\n",
    "                distance_range_max = 0\n",
    "                \n",
    "                \n",
    "                avg_depth = int(ceil(average_depth))\n",
    "                if avg_depth <= 8:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 3\n",
    "                elif avg_depth > 8 and avg_depth <= 10:\n",
    "                    distance_range_min = 3\n",
    "                    distance_range_max = 2\n",
    "                elif avg_depth > 10 and avg_depth <= 15:\n",
    "                    distance_range_min = 4\n",
    "                    distance_range_max = 3\n",
    "                elif avg_depth > 15 and avg_depth <= 20:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                elif avg_depth > 20 and avg_depth <= 30:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 2\n",
    "                elif avg_depth > 30 and avg_depth <= 40:\n",
    "                    distance_range_min = 8\n",
    "                    distance_range_max = 9\n",
    "                elif avg_depth > 40 and avg_depth <= 50:\n",
    "                    distance_range_min = 5\n",
    "                    distance_range_max = 7\n",
    "                elif avg_depth > 50 and avg_depth <= 60:\n",
    "                    distance_range_min = 5\n",
    "                    distance_range_max = 3\n",
    "                else:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                    \n",
    "                depth_min_range = average_depth - distance_range_min\n",
    "                depth_max_range = average_depth + distance_range_max\n",
    "                \n",
    "                # Checking the threshold values selected\n",
    "                #print(\"Threshold Values: \",depth_min_range,\" \",depth_max_range)\n",
    "                \n",
    "                # Checking the difference between the bbox_depth and averages depth value\n",
    "                #print(\"Difference\")\n",
    "                #print((bbox_depth - average_depth))\n",
    "                \n",
    "                # looking at the image of the difference\n",
    "                #plt.imshow((bbox_depth - average_depth), cmap = 'gray')\n",
    "                #plt.show()     \n",
    "\n",
    "                # Using Numpy to mask matrix positions that are within range.\n",
    "                box_mask = ma.masked_inside(bbox_depth, depth_min_range, depth_max_range)\n",
    "                \n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(mask1,cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(mask1)\n",
    "                \n",
    "                \n",
    "                box_mask_2 = box_mask.mask.astype(int)\n",
    "\n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(box_mask_2,cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(box_mask_2)\n",
    "                \n",
    "                box_mask_2[box_mask_2 == 0] = 255\n",
    "                box_mask_2[box_mask_2 == 1] = 0\n",
    "                \n",
    "                #image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1] = int_mask\n",
    "                \n",
    "                # Uncomment below code for checking the image\n",
    "                #plt.imshow(box_mask_2,cmap='gray')\n",
    "                #plt.show()\n",
    "                \n",
    "                #print(\"Mask section Shape: \",box_mask_2.shape)\n",
    "                #print(box_mask_2)               \n",
    "                \n",
    "                \n",
    "                mask_shape = box_mask_2.shape\n",
    "                #print(mask_shape)\n",
    "                \n",
    "\n",
    "                check = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()\n",
    "                box_mask_2 = box_mask_2.flatten()\n",
    "                image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1] = np.array([b if a!=0 else a for a,b in zip(check,box_mask_2)]).reshape(mask_shape)\n",
    "                gt = gt_seg[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()\n",
    "                est = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()          \n",
    "                precision = map_precision(est, gt)\n",
    "                recall = map_recall(est, gt)\n",
    "                \n",
    "                print(\"Current detection precision is: \",precision)\n",
    "                print(\"Current detection recall is: \",recall)\n",
    "\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                count += 1\n",
    "                #break\n",
    "                print(\" \")\n",
    "                \n",
    "        p_score = total_precision/count\n",
    "        r_score = total_recall/count\n",
    "                \n",
    "        #plt.imshow(image_mask,cmap='gray')\n",
    "        #plt.show()\n",
    "        \n",
    "        print(\"Overall Precision \",p_score ,\"Overall recall: \",r_score,\"\\n\" )\n",
    "        \n",
    "            \n",
    "        # Save the segmentation mask\n",
    "        cv2.imwrite(os.path.join(output_dir,sample_name + \".png\"),image_mask)\n",
    "        \n",
    "        \n",
    "        '''        \n",
    "        obj.type = labels[obj_idx, 0]\n",
    "        obj.truncation = float(labels[obj_idx, 1])\n",
    "        obj.occlusion = float(labels[obj_idx, 2])\n",
    "        obj.alpha = float(labels[obj_idx, 3])\n",
    "\n",
    "        obj.x1, obj.y1, obj.x2, obj.y2 = (labels[obj_idx, 4:8]).astype(np.float32)\n",
    "        obj.h, obj.w, obj.l = (labels[obj_idx, 8:11]).astype(np.float32)\n",
    "        obj.t = (labels[obj_idx, 11:14]).astype(np.float32)\n",
    "        obj.ry = float(labels[obj_idx, 14])\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    train_main1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part3: Training Code Method 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_main2():\n",
    "\n",
    "    ################\n",
    "    # Options\n",
    "    ################\n",
    "    # Input dir and output dir\n",
    "    \n",
    "   \n",
    "    depth_dir = 'data\\\\train\\\\est_depth'\n",
    "    label_dir = 'data\\\\train\\\\gt_labels'\n",
    "    image_dir = 'data\\\\train\\\\left'\n",
    "    output_dir = 'data\\\\train\\\\est_segmentation'\n",
    "    gt_seg_dir = 'data\\\\train\\\\gt_segmentation'\n",
    "    #sample_list = ['000011', '000012', '000013', '000014', '000015']\n",
    "    sample_list = ['000001', '000002', '000003', '000004','000005', '000006', '000007', '000008', '000009', '000010']\n",
    "    #sample_list = ['000009']\n",
    "    ################\n",
    "\n",
    "    for sample_name in sample_list:\n",
    "        \n",
    "        # Read depth map\n",
    "        depth_path = depth_dir+'\\\\'+sample_name+'.png'\n",
    "        depth_map =  cv2.imread(depth_path,cv2.IMREAD_GRAYSCALE)\n",
    "        #print(depth_map)\n",
    "        #break\n",
    "        \n",
    "        # Read Ground Truth Segmentation Image:\n",
    "        gt_seg_path  = gt_seg_dir+'\\\\'+sample_name+'.png'\n",
    "        gt_seg = cv2.imread(gt_seg_path,cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #print(gt_seg.shape)\n",
    "        \n",
    "        #image = cv2.imread(os.path.join(image_dir,sample_name+\".png\"))\n",
    "        #image_mask = np.ones(image.shape[:2])*255\n",
    "        \n",
    "        print(sample_name)\n",
    "        precision_scores = 0\n",
    "        recall_scores = 0\n",
    "        count = 0\n",
    "        # Discard depths less than 10cm from the camera\n",
    "        depth_map[depth_map < 0.1] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        distance_range = 3\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Read 2d bbox\n",
    "        bbox = read_labels(label_dir, sample_name)\n",
    "        image_mask = np.ones_like(depth_map) * 255\n",
    "        # For each bbox\n",
    "        for label in bbox:\n",
    "            \n",
    "            if label.type == 'Car':\n",
    "                #print(sample_name+\" \"+label.type)\n",
    "                \n",
    "                # Estimate the average depth of the objects\n",
    "                \n",
    "                bbox_depth = depth_map[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1]\n",
    "\n",
    "                #average_depth = np.mean(bbox_depth, axis=(0 , 1))\n",
    "                average_depth = np.nanmean(np.nanmean(np.where(bbox_depth!=0,bbox_depth,np.nan),1))\n",
    "                print(\"Average Depth: \",average_depth)\n",
    "                \n",
    "                # defing a distance for image segmentation\n",
    "                distance_range = 10\n",
    "                \n",
    "                \n",
    "                avg_depth = int(ceil(average_depth))\n",
    "                distance_range_min = 0\n",
    "                distance_range_max = 0\n",
    "                \n",
    "                if avg_depth <= 8:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 3\n",
    "                elif avg_depth > 8 and avg_depth <= 10:\n",
    "                    distance_range_min = 1\n",
    "                    distance_range_max = 2\n",
    "                elif avg_depth > 10 and avg_depth <= 15:\n",
    "                    distance_range_min = 15\n",
    "                    distance_range_max = 1\n",
    "                elif avg_depth > 15 and avg_depth <= 20:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 5\n",
    "                elif avg_depth > 20 and avg_depth <= 30:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 2\n",
    "                elif avg_depth > 30 and avg_depth <= 40:\n",
    "                    distance_range_min = 8\n",
    "                    distance_range_max = 9\n",
    "                elif avg_depth > 40 and avg_depth <= 50:\n",
    "                    distance_range_min = 5\n",
    "                    distance_range_max = 7\n",
    "                elif avg_depth > 50 and avg_depth <= 60:\n",
    "                    distance_range_min = 2\n",
    "                    distance_range_max = 3\n",
    "                else:\n",
    "                    distance_range_min = 10\n",
    "                    distance_range_max = 15\n",
    "                \n",
    "                \n",
    "                # Calculating Depth Threshold Value\n",
    "                \n",
    "                depth_min_range = average_depth-distance_range_min\n",
    "                depth_max_range = average_depth+distance_range_max\n",
    "                \n",
    "                # print(\"Threshold Values: \",depth_threshold_min,\" \",depth_threshold_max)\n",
    "                \n",
    "                \n",
    "                # Checking different options for best precision\n",
    "                \n",
    "                #box_y, box_x = np.where(np.abs(bbox_depth - average_depth) < distance_range)\n",
    "                #box_y, box_x = np.where(np.logical_and((bbox_depth - average_depth) < distance_range_max,(bbox_depth - average_depth) > distance_range_min ))\n",
    "                \n",
    "                # using the numpy where method to get the x,y indices for the martix that meet the criteria\n",
    "                box_y, box_x = np.where(np.logical_and((bbox_depth - average_depth) < depth_max_range,(bbox_depth - average_depth) > depth_min_range ))\n",
    "                \n",
    "                \n",
    "                image_mask[box_y + int(label.y1), box_x + int(label.x1)] = 0\n",
    "                box_mask =  image_mask[box_y + int(label.y1), box_x + int(label.x1)]\n",
    "                \n",
    "                \n",
    "                #print(\"Mask Image Shape: \",image_mask.shape)             \n",
    "                #a = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1]\n",
    "                #plt.imshow(a, cmap = 'gray')\n",
    "                #plt.show()\n",
    "                \n",
    "                               \n",
    "                gt = gt_seg[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()\n",
    "                est = image_mask[int(label.y1):int(label.y2)+1, int(label.x1):int(label.x2)+1].flatten()          \n",
    "                \n",
    "                precision = map_precision(est, gt)\n",
    "                print(\"Current detection precision is: \",precision)\n",
    "                \n",
    "                recall = map_recall(est, gt)\n",
    "                \n",
    "                \n",
    "                precision_scores += precision\n",
    "                recall_scores += recall\n",
    "                count += 1\n",
    "                \n",
    "                #break\n",
    "                print(\" \")\n",
    "        p_score = precision_scores/count\n",
    "        r_score = recall_scores/count\n",
    "                \n",
    "        #plt.imshow(image_mask,cmap='gray')\n",
    "        #plt.show()\n",
    "        \n",
    "        print(\"Overall Precision \",p_score ,\"Overall recall: \",r_score,\"\\n\" )\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    train_main2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
